# -*- coding: utf-8 -*-
"""Analyising

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LLMT94xrPm4_UiVqjUU6aENp40GiQGBr
"""

import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical computations
import matplotlib.pyplot as plt  # For creating visualizations
import seaborn as sns  # For statistical data visualization
import sklearn #scikit-learn is aliased as sklearn in python 3. In python2, the library is just sklearn

from google.colab import drive
drive.mount('/content/drive')
# Example: Accessing a file in your 'My Drive' folder
file_path1 = '/content/drive/My Drive/Project/Quantics/Dataset/email_opened_table.csv'
filepath2= '/content/drive/My Drive/Project/Quantics/Dataset/email_table.csv'
filepath3='/content/drive/My Drive/Project/Quantics/Dataset/link_clicked_table.csv' # Replace with your file's path# Assuming it's a CSV file, adjust as needed

df_emailtable = pd.read_csv(filepath2)
df_emailopened= pd.read_csv(file_path1)
df_linkopened=pd.read_csv(filepath3)
df_emailtable.head()

df_emailopened.head()

df_linkopened.head()

df_emailtable.info()

df_emailtable['Email opened'] = df_emailtable['email_id'].isin(df_emailopened['email_id']).map({True: 'Yes', False: 'No'})

df_emailtable['Link opened'] = df_emailtable['email_id'].isin(df_linkopened['email_id']).map({True: 'Yes', False: 'No'})

df_emailtable.head()

# Total people who got the email
total_sent = len(df_emailtable)

# Total people who opened the email
total_opened = (df_emailtable['Email opened'] == 'Yes').sum()

# Total people who opened the link
link_opened = (df_emailtable['Link opened'] == 'Yes').sum()

# Plotting
labels = ['Email Sent', 'Email Opened','Link opened']
values = [total_sent, total_opened,link_opened]

plt.figure(figsize=(6, 4))
bars = plt.bar(labels, values, color=['skyblue', 'lightgreen'])

# Add numbers above bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 5, int(yval), ha='center', va='bottom', fontsize=12)

plt.title('Email Sent vs Opened')
plt.ylabel('Number of Users')
plt.ylim(0, max(values) * 1.1)
plt.tight_layout()
plt.show()

# Count of opened and not opened
opened_count = (df_emailtable['Email opened'] == 'Yes').sum()
not_opened_count = (df_emailtable['Email opened'] == 'No').sum()

# Labels and sizes
labels = ['Opened', 'Not Opened']
sizes = [opened_count, not_opened_count]
colors = ['lightgreen', 'lightcoral']

# Plotting
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Email Open Rate')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.tight_layout()
plt.show()

# Count of opened and not opened
opened_count = (df_emailtable['Link opened'] == 'Yes').sum()
not_opened_count = (df_emailtable['Link opened'] == 'No').sum()

# Labels and sizes
labels = ['Opened', 'Not Opened']
sizes = [opened_count, not_opened_count]
colors = ['lightgreen', 'lightcoral']

# Plotting
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Link Open Rate')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.tight_layout()
plt.show()

# prompt: piechart for links opened out of emails opened

# Count of link opened out of emails opened
link_opened_from_opened = df_emailtable[(df_emailtable['Email opened'] == 'Yes') & (df_emailtable['Link opened'] == 'Yes')].shape[0]
link_not_opened_from_opened = df_emailtable[(df_emailtable['Email opened'] == 'Yes') & (df_emailtable['Link opened'] == 'No')].shape[0]


# Labels and sizes
labels = ['Link Opened', 'Link Not Opened']
sizes = [link_opened_from_opened, link_not_opened_from_opened]
colors = ['lightgreen', 'lightcoral']

# Plotting
plt.figure(figsize=(6, 6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Link Opened Rate (from Opened Emails)')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.tight_layout()
plt.show()

uniquecountries = df_emailtable['user_country'].unique()
print(uniquecountries)

df_emailtable.head()



# Total people who opened the email
opened_count = (df_emailtable['Email opened'] == 'Yes').sum()

# People who opened the email AND clicked the link
clicked_and_opened = df_emailtable[
    (df_emailtable['Email opened'] == 'Yes') & (df_emailtable['Link opened'] == 'Yes')
].shape[0]

# Calculate Click-to-Open Rate
if opened_count > 0:
    ctor = (clicked_and_opened / opened_count) * 100
else:
    ctor = 0

print(f"Click-to-Open Rate (CTOR): {ctor:.2f}%")

# Labels and sizes
labels = ['Link Opened when opened email', 'Link not opened when email opened']
sizes = [ctor, 100-ctor]
colors = ['lightgreen', 'lightcoral']

# Plotting
plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)  # Change startangle to 90
plt.title('Click through rate')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.tight_layout()
plt.show()

uniquemailversion = df_emailtable['email_version'].unique()
print(uniquemailversion)

# Filter data
personalized_opened = df_emailtable[
    (df_emailtable['email_version'] == 'personalized') & (df_emailtable['Email opened'] == 'Yes')
].shape[0]

generic_opened = df_emailtable[
    (df_emailtable['email_version'] == 'generic') & (df_emailtable['Email opened'] == 'Yes')
].shape[0]

# Bar chart
labels = ['Personalized Emails Opened', 'Generic Emails Opened']
values = [personalized_opened, generic_opened]
colors = ['mediumseagreen', 'steelblue']

plt.figure(figsize=(7, 4))
bars = plt.bar(labels, values, color=colors)

# Add values above bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 5, int(yval), ha='center', va='bottom', fontsize=12)

plt.title('Email Opens by Email Version')
plt.ylabel('Number of Users')
plt.tight_layout()
plt.show()

# Total personalized emails sent
total_personalized = df_emailtable[df_emailtable['email_version'] == 'personalized'].shape[0]

# Total generic emails sent
total_generic = df_emailtable[df_emailtable['email_version'] == 'generic'].shape[0]

# Personalized opened
opened_personalized = df_emailtable[
    (df_emailtable['email_version'] == 'personalized') & (df_emailtable['Email opened'] == 'Yes')
].shape[0]

# Generic opened
opened_generic = df_emailtable[
    (df_emailtable['email_version'] == 'generic') & (df_emailtable['Email opened'] == 'Yes')
].shape[0]

# Calculate open rates
personalized_open_rate = (opened_personalized / total_personalized) * 100 if total_personalized > 0 else 0
generic_open_rate = (opened_generic / total_generic) * 100 if total_generic > 0 else 0

labels = ['Personalized', 'Generic']
open_rates = [personalized_open_rate, generic_open_rate]
colors = ['mediumseagreen', 'steelblue']

plt.figure(figsize=(6, 4))
bars = plt.bar(labels, open_rates, color=colors)

# Add percentage values on top
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 1, f"{yval:.2f}%", ha='center', va='bottom', fontsize=12)

plt.title('Email Open Rate by Email Version')
plt.ylabel('Open Rate (%)')
plt.ylim(0, max(open_rates) * 1.2)
plt.tight_layout()
plt.show()

print(f"Total personalized emails sent: {total_personalized}")
print(f"Total generic emails sent: {total_generic}")
print(f"Personalized Emails Opened: {personalized_opened}")
print(f"Generic Emails Opened: {generic_opened}")

#Number of people who got short mail
total_shortmail=sum(df_emailtable['email_text']=="short_email")

#Number of people who got long mail
total_longmail=sum(df_emailtable['email_text']=="long_email")

#Number of people who got short mail and opened the mail
total_shortmailopen=sum((df_emailtable['email_text']=="short_email")&(df_emailtable['Email opened']=="Yes"))

#Number of people who got long mail and opened the mail
total_longmailopen=sum((df_emailtable['email_text']=="long_email")&(df_emailtable['Email opened']=="Yes"))

#Long mail link opened
total_longmaillinkopen=sum((df_emailtable['email_text']=="long_email")&(df_emailtable['Link opened']=="Yes"))

#short mail link opened
total_shortmaillinkopen=sum((df_emailtable['email_text']=="short_email")&(df_emailtable['Link opened']=="Yes"))

labels = ['Short Mail Sent', 'Short Mail Opened', 'Short mail link clicked', 'Long Mail Sent', 'Long Mail Opened', 'Long Mail link clicked']
values = [total_shortmail, total_shortmailopen, total_shortmaillinkopen, total_longmail, total_longmailopen, total_longmaillinkopen]
colors = ['blue', 'lightblue', 'cyan', 'red', 'lightcoral', 'orange']  # Adjusted colors for 6 bars

plt.figure(figsize=(10, 6))  # Increased figure size for better readability
bars = plt.bar(labels, values, color=colors)

# Add count values on top
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 10, int(yval), ha='center', va='bottom', fontsize=10)

plt.title('Email Sent, Opened, and Link Clicked by Email Length')
plt.ylabel('Number of Emails')
plt.ylim(0, max(values) * 1.2)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

totalemails_sent = df_emailtable.count()["email_id"]
totalemails_opened = df_emailtable[df_emailtable["Email opened"] == "Yes"].count()["email_id"]
unopened_emails = totalemails_sent-totalemails_opened

#open rate
open_rate=(totalemails_opened/totalemails_sent)*100
print("Total open rate = ",open_rate)

#conversion rate
goal_completed=df_emailtable[df_emailtable["Link opened"] == "Yes"].count()["email_id"]
conversion_rate = (goal_completed/totalemails_sent)*100
print("Total conversion rate = ",conversion_rate)

#Now trying to figure out if personalized did better than generic
totalpersonalizedemails_sent = df_emailtable[df_emailtable["email_version"] == "personalized"].count()["email_id"]
totalpersonalizedemails_opened = df_emailtable[(df_emailtable["email_version"] == "personalized") & (df_emailtable["Email opened"] == "Yes")].count()["email_id"]
personalized_open_rate=(totalpersonalizedemails_opened/totalpersonalizedemails_sent)*100
print("Total personalized open rate = ",personalized_open_rate)

totalgenericemails_sent = df_emailtable[df_emailtable["email_version"] == "generic"].count()["email_id"]
totalgenericemails_opened = df_emailtable[(df_emailtable["email_version"] == "generic") & (df_emailtable["Email opened"] == "Yes")].count()["email_id"]
generic_open_rate=(totalgenericemails_opened/totalgenericemails_sent)*100
print("Total generic open rate = ",generic_open_rate)

delta_open=(personalized_open_rate-generic_open_rate)/generic_open_rate*100
print("Personalized emails performed better than generic emails by ",delta_open)

countries = ['US', 'UK', 'FR', 'ES']
email_data = []

for country in countries:
    # Emails sent
    sent_count = df_emailtable[df_emailtable['user_country'] == country].shape[0]

    # Emails opened
    opened_count = df_emailtable[(df_emailtable['user_country'] == country) & (df_emailtable['Email opened'] == 'Yes')].shape[0]

    # Links clicked
    clicked_count = df_emailtable[(df_emailtable['user_country'] == country) & (df_emailtable['Link opened'] == 'Yes')].shape[0]

    # Calculate open rate
    open_rate = (opened_count / sent_count) * 100 if sent_count > 0 else 0

    # Calculate conversion rate
    conversion_rate = (clicked_count / sent_count) * 100 if sent_count > 0 else 0

    email_data.append([country, sent_count, opened_count, clicked_count, open_rate, conversion_rate])

# Create a DataFrame from the collected data
email_df = pd.DataFrame(email_data, columns=['Country', 'Emails Sent', 'Emails Opened', 'Links Clicked', 'Open Rate', 'Conversion Rate'])
print(email_df)

# Data from the DataFrame
countries = email_df['Country']
sent = email_df['Emails Sent']
opened = email_df['Emails Opened']
clicked = email_df['Links Clicked']

# Set width of bar
barWidth = 0.25

# Set position of bar on X axis
r1 = np.arange(len(countries))
r2 = [x + barWidth for x in r1]
r3 = [x + barWidth for x in r2]

# Make the plot
plt.bar(r1, sent, color='#7f6d5f', width=barWidth, edgecolor='white', label='Emails Sent')
plt.bar(r2, opened, color='#557f2d', width=barWidth, edgecolor='white', label='Emails Opened')
plt.bar(r3, clicked, color='#2d7f5e', width=barWidth, edgecolor='white', label='Links Clicked')

# Add xticks at the center of the group bars
plt.xlabel('Country', fontweight='bold')
plt.xticks([r + barWidth for r in range(len(countries))], countries)

# Create legend & Show graphic
plt.legend()
plt.show()



import pandas as pd

# Assuming your DataFrame is named 'df_emailtable'
purchase_counts = df_emailtable['user_past_purchases'].value_counts()
print(purchase_counts)



import pandas as pd

# Assuming your DataFrame is named 'df_emailtable'

# Group by 'user_past_purchases' and 'email_version' and count occurrences
email_version_counts = df_emailtable.groupby(['user_past_purchases', 'email_version'])['email_id'].count().reset_index()

# Rename the count column for clarity
email_version_counts.rename(columns={'email_id': 'count'}, inplace=True)

# Print the results
print(email_version_counts)



import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns  # Import seaborn for better styling

# ... (previous code to calculate email_version_counts)

# Create a grouped bar chart
plt.figure(figsize=(10, 6))  # Adjust figure size as needed
sns.barplot(x='user_past_purchases', y='count', hue='email_version', data=email_version_counts)
plt.title('Email Version Distribution by Past Purchases')
plt.xlabel('Past Purchases')
plt.ylabel('Number of Emails')
plt.legend(title='Email Version')
plt.show()



import pandas as pd

# Assuming your DataFrame is named 'df_emailtable'

# Get unique values in the 'hour' column
unique_hours = df_emailtable['hour'].unique()
print("Unique values in 'hour' column:", unique_hours)

# Get unique values in the 'weekday' column
unique_weekdays = df_emailtable['weekday'].unique()
print("Unique values in 'weekday' column:", unique_weekdays)

# Group by 'weekday' and 'hour' and calculate the counts of opened and clicked emails
weekday_hour_stats = df_emailtable.groupby(['weekday', 'hour']).agg(
    total_emails=('email_id', 'count'),  # Total emails sent for this weekday/hour
    opened_emails=('Email opened', lambda x: (x == 'Yes').sum()),  # Opened emails count
    clicked_emails=('Link opened', lambda x: (x == 'Yes').sum())  # Clicked emails count
).reset_index()  # Reset the index to get weekday and hour as columns

# Calculate open and click rates
weekday_hour_stats['open_rate'] = (weekday_hour_stats['opened_emails'] / weekday_hour_stats['total_emails']) * 100
weekday_hour_stats['click_rate'] = (weekday_hour_stats['clicked_emails'] / weekday_hour_stats['total_emails']) * 100

# Display the results, sorted by open rate or click rate if desired
print(weekday_hour_stats.sort_values(by=['open_rate', 'click_rate'], ascending=False))

# Display the top 10 results sorted by open rate
print(weekday_hour_stats.sort_values(by=['open_rate'], ascending=False).head(10))

# Assuming you have 'weekday_hour_stats' DataFrame from previous steps

# Pivot the data for heatmap using all values
heatmap_data = weekday_hour_stats.pivot(index="weekday", columns="hour", values="open_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Open Rates", fontsize=16)  # Increased title font size
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

# Assuming you have 'weekday_hour_stats' DataFrame from previous steps

# Pivot the data for heatmap using all values
heatmap_data = weekday_hour_stats.pivot(index="weekday", columns="hour", values="click_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Open Rates", fontsize=16)  # Increased title font size
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have 'weekday_hour_stats' DataFrame from previous steps

# Filter data to include only combinations with more than 100 total emails
filtered_data = weekday_hour_stats[weekday_hour_stats['total_emails'] > 100]

# Pivot the data for heatmap using the filtered data
heatmap_data = filtered_data.pivot(index="weekday", columns="hour", values="open_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Open Rates (Total Emails > 100)", fontsize=16)  # Updated title
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have 'weekday_hour_stats' DataFrame from previous steps

# Filter data to include only combinations with more than 100 total emails
filtered_data = weekday_hour_stats[weekday_hour_stats['total_emails'] > 100]

# Pivot the data for heatmap using the filtered data
heatmap_data = filtered_data.pivot(index="weekday", columns="hour", values="click_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Open Rates (Total Emails > 100)", fontsize=16)  # Updated title
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have 'df_emailtable' and 'weekday_hour_stats' DataFrames

# Filter for personalized emails
personalized_emails = df_emailtable[df_emailtable['email_version'] == 'personalized']

# Group by 'weekday' and 'hour' and calculate the counts of opened and clicked emails for personalized emails
personalized_weekday_hour_stats = personalized_emails.groupby(['weekday', 'hour']).agg(
    total_emails=('email_id', 'count'),  # Total emails sent for this weekday/hour
    clicked_emails=('Link opened', lambda x: (x == 'Yes').sum())  # Clicked emails count
).reset_index()  # Reset the index to get weekday and hour as columns

# Calculate click rates for personalized emails
personalized_weekday_hour_stats['click_rate'] = (personalized_weekday_hour_stats['clicked_emails'] / personalized_weekday_hour_stats['total_emails']) * 100

# Filter data to include only combinations with more than 100 total emails
filtered_data = personalized_weekday_hour_stats[personalized_weekday_hour_stats['total_emails'] > 100]

# Pivot the data for heatmap using the filtered data
heatmap_data = filtered_data.pivot(index="weekday", columns="hour", values="click_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Click Rates for Personalized Emails (Total Emails > 100)", fontsize=16)  # Updated title
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have 'df_emailtable' and 'weekday_hour_stats' DataFrames

# Filter for generic emails
generic_emails = df_emailtable[df_emailtable['email_version'] == 'generic']

# Group by 'weekday' and 'hour' and calculate the counts of opened and clicked emails for generic emails
generic_weekday_hour_stats = generic_emails.groupby(['weekday', 'hour']).agg(
    total_emails=('email_id', 'count'),  # Total emails sent for this weekday/hour
    clicked_emails=('Link opened', lambda x: (x == 'Yes').sum())  # Clicked emails count
).reset_index()  # Reset the index to get weekday and hour as columns

# Calculate click rates for generic emails
generic_weekday_hour_stats['click_rate'] = (generic_weekday_hour_stats['clicked_emails'] / generic_weekday_hour_stats['total_emails']) * 100

# Filter data to include only combinations with more than 100 total emails
filtered_data = generic_weekday_hour_stats[generic_weekday_hour_stats['total_emails'] > 100]

# Pivot the data for heatmap using the filtered data
heatmap_data = filtered_data.pivot(index="weekday", columns="hour", values="click_rate")

# Create the heatmap with adjustments for better readability
plt.figure(figsize=(16, 8))  # Increased figure size for more space
sns.heatmap(heatmap_data, annot=True, cmap="YlGnBu", fmt=".2f",
            linewidths=0.5, annot_kws={"size": 10})  # Added linewidths and annotation size
plt.title("Weekday/Hour Combinations with Click Rates for Generic Emails (Total Emails > 100)", fontsize=16)  # Updated title
plt.xlabel("Hour", fontsize=12)  # Increased x-axis label font size
plt.ylabel("Weekday", fontsize=12)  # Increased y-axis label font size
plt.xticks(fontsize=10)  # Increased x-axis tick label font size
plt.yticks(fontsize=10)  # Increased y-axis tick label font size
plt.tight_layout()  # Adjusts layout to prevent overlapping elements
plt.show()

# Required imports
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
import pandas as pd

# Step 1: Features and target
features = ['email_text', 'email_version', 'hour', 'weekday', 'user_country', 'user_past_purchases']
target = 'Email opened'

X = df_emailtable[features]
y = df_emailtable[target].map({'Yes': 1, 'No': 0})

# Step 2: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Preprocessing
num_cols = ['hour', 'user_past_purchases']
cat_cols = ['email_text', 'email_version', 'weekday', 'user_country']

numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

preprocessor = ColumnTransformer([
    ('num', numeric_transformer, num_cols),
    ('cat', categorical_transformer, cat_cols)
])

# Step 4: Preprocess training data
X_train_transformed = preprocessor.fit_transform(X_train)

# Step 5: Apply SMOTE
sm = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = sm.fit_resample(X_train_transformed, y_train)

# Step 6: Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_balanced, y_train_balanced)

# Step 7: Preprocess and predict test set
X_test_transformed = preprocessor.transform(X_test)
y_pred = rf_model.predict(X_test_transformed)

# Step 8: Evaluation
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# 1. Separate features and target
X = df_emailtable.drop('Email opened', axis=1)
y = df_emailtable['Email opened']

# 2. Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. Define all categorical columns (including any Yes/No or string-based columns)
categorical_cols = ['email_text', 'email_version', 'hour', 'weekday', 'user_country', 'user_past_purchases']

# 4. Preprocessing: One-hot encode all categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ],
    remainder='drop'  # Drop everything else that is not encoded
)

# 5. Transform training and test sets
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# 6. Apply SMOTE for oversampling the minority class
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_transformed, y_train)

# 7. Define the hyperparameter grid for RandomForest
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 3],
    'class_weight': ['balanced']
}

# 8. Create the base model
rf = RandomForestClassifier(random_state=42)

# 9. Grid search with cross-validation
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,
    scoring='f1',
    verbose=2,
    n_jobs=-1
)

# 10. Fit the model on balanced data
grid_search.fit(X_train_balanced, y_train_balanced)

# 11. Get best model and evaluate
best_rf = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# 12. Predict on test data
y_pred = best_rf.predict(X_test_transformed)

# 13. Display results
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

df_emailtable.head()

df_emailtable['email_text_binary'] = df_emailtable['email_text'].apply(lambda x: 0 if x == 'short_email' else 1)

df_emailtable['email_version_binary'] = df_emailtable['email_version'].apply(lambda x: 0 if x == 'generic' else 1)

df_emailtable['working_hour'] = df_emailtable.apply(
    lambda row: 1 if row['weekday'] in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'] and 9 <= row['hour'] <= 17 else 0,
    axis=1
)

df_emailtable.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Countplot of working_hour vs Email opened
plt.figure(figsize=(6,4))
sns.countplot(data=df_emailtable, x='working_hour', hue='Email opened', palette='Set2')
plt.title('Email Opened vs Working Hour')
plt.xlabel('Working Hour (0 = Off Hours, 1 = Working Hours)')
plt.ylabel('Count')
plt.legend(title='Email Opened')
plt.tight_layout()
plt.show()
#so now we know that according to the graph, if it is a non working hour people are way likey to not open their email

# Count of emails per country
country_counts = df_emailtable['user_country'].value_counts()

# Display the result
print(country_counts)

def segment_user(purchases):
    if purchases == 0:
        return 'New'
    elif 1 <= purchases <= 3:
        return 'Low'
    elif 4 <= purchases <= 10:
        return 'Mid'
    else:
        return 'High'

df_emailtable['purchase_segment'] = df_emailtable['user_past_purchases'].apply(segment_user)

df_emailtable.head()

# Count of users in each purchase segment
segment_counts = df_emailtable['purchase_segment'].value_counts()

# Display the result
print(segment_counts)
plt.figure(figsize=(6,4))
sns.barplot(x=segment_counts.index, y=segment_counts.values, palette='pastel')
plt.title('User Count per Purchase Segment')
plt.xlabel('Purchase Segment')
plt.ylabel('Number of Users')
plt.tight_layout()
plt.show()

# Map purchase_segment to numeric values
segment_mapping = {'New': 0, 'Low': 1, 'Mid': 2, 'High': 3}
df_emailtable['purchase_segment_numeric'] = df_emailtable['purchase_segment'].map(segment_mapping)

# Convert 'Email opened' and 'Link opened' to binary if they aren’t already
df_emailtable['Email opened binary'] = df_emailtable['Email opened'].apply(lambda x: 1 if x == 'Yes' else 0)
df_emailtable['Link opened binary'] = df_emailtable['Link opened'].apply(lambda x: 1 if x == 'Yes' else 0)

# Correlation with purchase_segment_numeric
email_corr = df_emailtable['purchase_segment_numeric'].corr(df_emailtable['Email opened binary'])
link_corr = df_emailtable['purchase_segment_numeric'].corr(df_emailtable['Link opened binary'])

print(f"Correlation between Purchase Segment and Email Opened: {email_corr:.4f}")
print(f"Correlation between Purchase Segment and Link Opened: {link_corr:.4f}")

# Convert target variable to binary
df_emailtable['Email opened binary'] = df_emailtable['Email opened'].apply(lambda x: 1 if x == 'Yes' else 0)

# Convert target variable to binary
df_emailtable['Email opened binary'] = df_emailtable['Email opened'].apply(lambda x: 1 if x == 'Yes' else 0)

# Verify that the 'Email opened binary' column is added correctly
print(df_emailtable.columns)  # Check if the new column is in the DataFrame

# Columns to exclude from correlation (target variable and non-numeric columns)
exclude_columns = ['Email opened', 'email_text', 'Link opened', 'email_version', 'user_country', 'purchase_segment', 'hour', 'weekday']

# Select numeric columns (including 'Email opened binary')
numeric_columns = df_emailtable.drop(columns=exclude_columns)

# Calculate the correlation matrix for all features with the target
correlation_matrix = numeric_columns.corr()

# Add target variable correlation to the matrix
email_corr_with_target = correlation_matrix['Email opened binary'].sort_values(ascending=False)

# Display the correlations
print(email_corr_with_target)

df_emailtable.head()

import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Assuming df_emailtable is your DataFrame and 'Email opened binary' is your target variable

# Define feature columns (excluding 'email_id' and 'Email opened binary' as they are not useful for modeling)
feature_columns = [
    'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric',
    'user_past_purchases'
]

# Selecting the features and the target variable
X = df_emailtable[feature_columns]
y = df_emailtable['Email opened binary']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instantiate the XGBoost model
model = xgb.XGBClassifier(
    objective='binary:logistic',  # Binary classification
    eval_metric='logloss',  # Log loss as the evaluation metric
    use_label_encoder=False,  # Avoid label encoder warnings
    scale_pos_weight=1  # Adjust based on class imbalance
)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Accuracy score
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Import necessary libraries
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import confusion_matrix, classification_report

# Assuming df_emailtable is your DataFrame
# Let's define the features (X) and target (y)

X = df_emailtable[['email_text_binary', 'email_version_binary', 'working_hour',
                   'purchase_segment_numeric']]
y = df_emailtable['Email opened binary']

# Step 1: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Apply SMOTE for oversampling the minority class in the training set
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Step 3: Initialize the XGBoost classifier and fit the model to the resampled data
model = xgb.XGBClassifier(scale_pos_weight=1, random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# Step 4: Predict on the test data
y_pred = model.predict(X_test)

# Step 5: Evaluate the model
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Display the confusion matrix and classification report
print("Confusion Matrix:")
print(conf_matrix)

print("\nClassification Report:")
print(class_report)

# Importing necessary libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# Assuming the dataset is df_emailtable and the target variable is 'Email opened binary'

# Encode categorical features (if any) into numerical values
le = LabelEncoder()
df_emailtable['email_text_binary'] = le.fit_transform(df_emailtable['email_text'])
df_emailtable['email_version_binary'] = le.fit_transform(df_emailtable['email_version'])
df_emailtable['user_country'] = le.fit_transform(df_emailtable['user_country'])

# Select features (X) and target (y)
X = df_emailtable[['email_text_binary', 'email_version_binary',
                   'user_country', 'user_past_purchases', 'working_hour',
                   'purchase_segment_numeric']]
y = df_emailtable['Email opened binary']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to oversample the minority class
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train a Gradient Boosting model
gb_model = GradientBoostingClassifier(random_state=42)

# Perform Grid Search for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(gb_model, param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train_resampled, y_train_resampled)

# Best parameters after grid search
print("Best parameters from GridSearchCV:", grid_search.best_params_)

# Use the best model found from GridSearch
best_gb_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_gb_model.predict(X_test)

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Ensure 'Email opened binary' is available in the dataframe
# df_emailtable['Email opened binary'] should already be created

# Feature selection (exclude non-numeric columns and target variables)
X = df_emailtable[['email_text_binary', 'email_version_binary',
                   'user_country', 'user_past_purchases', 'working_hour',
                   'purchase_segment_numeric']]

# Target variable
y = df_emailtable['Email opened binary']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Build the neural network model
model = Sequential()

# Input layer
model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))

# Hidden layer(s)
model.add(Dense(units=32, activation='relu'))

# Output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)

# Predict on the test set
y_pred = (model.predict(X_test_scaled) > 0.5).astype("int32")

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}

# Build the neural network model
model = Sequential()

# Input layer
model.add(Dense(units=64, activation='relu', input_dim=X_train_scaled.shape[1]))

# Hidden layer(s)
model.add(Dense(units=32, activation='relu'))

# Output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model with class weights
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2, class_weight=class_weight_dict, verbose=1)

# Predict on the test set
y_pred = (model.predict(X_test_scaled) > 0.5).astype("int32")

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report

# Separate the features and target
X = df_emailtable.drop(columns=['Email opened', 'Link opened', 'email_text', 'user_country', 'purchase_segment', 'Email opened binary', 'Link opened binary','hour','weekday','email_version','email_text'])
y = df_emailtable['Email opened binary']

# Standardize the data (SVM performs better with scaled features)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize the SVM model
svm_model = SVC(kernel='linear', class_weight='balanced')  # 'linear' kernel for simplicity

# Train the model
svm_model.fit(X_train, y_train)

# Make predictions
y_pred = svm_model.predict(X_test)

# Evaluate the model
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

df_emailtable.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's pipeline for compatibility

# Define features and target
features = ['user_country', 'Email opened binary', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric']
target = 'Link opened binary'

X = df_emailtable[features]
y = df_emailtable[target]

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encode the categorical column (user_country)
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ],
    remainder='passthrough'  # Keep numerical columns as they are
)

# SMOTE oversampling
smote = SMOTE(random_state=42)

# Create pipeline using imblearn
clf = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', smote),  # Oversample the minority class
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])

# Train the model
clf.fit(X_train, y_train)

# Predict
y_pred = clf.predict(X_test)

# Evaluation
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

#prediction

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, roc_curve
from imblearn.over_sampling import SMOTE

# Define features and target
features = ['user_country', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']
target = 'Link opened binary'

X = df_emailtable[features]
y = df_emailtable[target]

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encode the categorical column (user_country)
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ],
    remainder='passthrough'
)

# Fit transform preprocessing on train only for oversampling
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Apply SMOTE to deal with class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_processed, y_train)

# Train the model
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_resampled, y_resampled)

# Predict probabilities
y_proba = model.predict_proba(X_test_processed)[:, 1]  # Probability of class '1' (link opened)

# Evaluate with ROC AUC
roc_auc = roc_auc_score(y_test, y_proba)
print(f"ROC AUC Score: {roc_auc:.4f}")

# Optional: Display a few predicted probabilities
print("\nSample predicted probabilities:")
print(y_proba[:10])
from sklearn.metrics import classification_report

# Evaluate the model
print(classification_report(y_test, model.predict(X_test_processed)))

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label='ROC Curve (AUC = {:.4f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Link Opened Prediction')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# Define the features and target
features = ['user_country', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']
target = 'Link opened binary'

# Split data
X = df_emailtable[features]
y = df_emailtable[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encoding and preprocessing
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
    remainder='passthrough'
)

# Create pipeline
model = Pipeline(steps=[('preprocessor', preprocessor),
                         ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))])

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Train the model
model.fit(X_resampled, y_resampled)

# Predict probabilities
y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability of class '1' (Link opened)

# Evaluate the model
roc_auc = roc_auc_score(y_test, y_pred_prob)
print(f"ROC AUC Score: {roc_auc:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, model.predict(X_test)))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, model.predict(X_test)))

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

# Define features and target
features = ['user_country', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']
target = 'Link opened binary'

X = df_emailtable[features]
y = df_emailtable[target]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing pipeline: OneHotEncoder for categorical data
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
    remainder='passthrough'
)

# Apply undersampling to deal with class imbalance
undersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = undersample.fit_resample(X_train, y_train)

# Create pipeline with RandomForest and preprocessor
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [5, 10, 20],
    'classifier__min_samples_split': [2, 5, 10],
}

grid_search = GridSearchCV(model, param_grid, cv=3, scoring='roc_auc')
grid_search.fit(X_resampled, y_resampled)

# Best model after tuning
best_model = grid_search.best_estimator_

# Predict probabilities
y_pred_prob = best_model.predict_proba(X_test)[:, 1]  # Probability of class '1' (Link opened)

# Evaluate performance
roc_auc = roc_auc_score(y_test, y_pred_prob)
print(f"ROC AUC Score: {roc_auc:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, best_model.predict(X_test)))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, best_model.predict(X_test)))

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.metrics import precision_recall_curve

# Define features and target
features = ['user_country', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']
target = 'Link opened binary'

X = df_emailtable[features]
y = df_emailtable[target]

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encode the categorical column (user_country)
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
    remainder='passthrough'
)

# Fit transform preprocessing on train only for oversampling
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Apply SMOTE to deal with class imbalance
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train_processed, y_train)

# RandomForest model with hyperparameter tuning
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__class_weight': ['balanced', None]
}

# Create a pipeline with the Random Forest model
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Use GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(clf, param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_resampled, y_resampled)

# Best model after hyperparameter tuning
best_model = grid_search.best_estimator_

# Predict probabilities
y_proba = best_model.predict_proba(X_test_processed)[:, 1]  # Probability of class '1' (link opened)

# Evaluate with ROC AUC
roc_auc = roc_auc_score(y_test, y_proba)
print(f"ROC AUC Score: {roc_auc:.4f}")

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Plot the Precision-Recall Curve
import matplotlib.pyplot as plt
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Calculate optimal threshold for precision-recall
optimal_idx = (precision + recall).argmax()
optimal_threshold = thresholds[optimal_idx]

# Predict with optimal threshold
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)

# Print classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred_optimal))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_optimal))

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

# Define features and target
features = ['user_country', 'email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']
target = 'Link opened binary'

X = df_emailtable[features]
y = df_emailtable[target]

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encode the categorical column (user_country)
categorical_features = ['user_country']
numerical_features = ['email_text_binary', 'email_version_binary', 'working_hour', 'purchase_segment_numeric', 'Email opened binary']

preprocessor = ColumnTransformer(
    transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],
    remainder='passthrough'
)

# Apply SMOTE to deal with class imbalance (only on training data)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Create a pipeline with preprocessing and classifier
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

# Define the parameter grid for RandomForestClassifier
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__class_weight': ['balanced', None]
}

# Use GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(clf, param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_resampled, y_resampled)

# Best model after hyperparameter tuning
best_model = grid_search.best_estimator_

# Predict probabilities
X_test_processed = preprocessor.transform(X_test)
y_proba = best_model.predict_proba(X_test_processed)[:, 1]  # Probability of class '1' (link opened)

# Evaluate with ROC AUC
roc_auc = roc_auc_score(y_test, y_proba)
print(f"ROC AUC Score: {roc_auc:.4f}")

# Precision-Recall Curve
precision, recall, thresholds = precision_recall_curve(y_test, y_proba)

# Plot the Precision-Recall Curve
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Calculate optimal threshold for precision-recall
optimal_idx = (precision + recall).argmax()
optimal_threshold = thresholds[optimal_idx]

# Predict with optimal threshold
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)

# Print classification report and confusion matrix
print("\nClassification Report:")
print(classification_report(y_test, y_pred_optimal))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_optimal))

df_emailtable.head()

# prompt: any duplicate entries in my df_emailtable email_id col

duplicate_emails = df_emailtable[df_emailtable.duplicated(subset=['email_id'], keep=False)]
if not duplicate_emails.empty:
    print("Duplicate email IDs found:")
    print(duplicate_emails)
else:
    print("No duplicate email IDs found.")

